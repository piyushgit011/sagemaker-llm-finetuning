# sagemaker-llm-finetuning-indic-languages
The problem with small models is that they donâ€™t have multilingual support. These models can answer only english languages perfectly, as the different languages like hindi ,etc takes more token to generate a single token.they have low accuracy with multilingual languages.

Now, we are finetuning the model for indic language support so we can generate responses with multilingual language with high accuracy.

Model:- https://huggingface.co/HuggingFaceH4/zephyr-7b-beta(finetned on mistral 7b)

In this sagemaker example, we are going to fine-tune open LLM, zephyr-7b beta using QLoRA(https://community.analyticsvidhya.com/c/generative-ai-tech-discussion/what-is-qlora)  and how to deploy them afterwards using the Hugging Face LLM Inference DLC.
In our example, we are going to leverage Hugging Face Transformers, Accelerate, and PEFT(https://github.com/huggingface/peft) . We will also make use of new and efficient features and methods including, Flash Attention,  and Mixed Precision Training.

Steps we are going to follow:

1. Setup Development Environment
2. Load and prepare the dataset
3. Fine-Tune zephyr 7B with QLoRA on Amazon SageMaker
4. Deploy Fine-tuned zephyr 7B on Amazon SageMaker
5. Stream Inference Requests from the Deployed Model using aws lambda.

# 1. Setup Development Environment:
If you are going to use zephyr-7b you need to login into our hugging face account, to use your token for accessing the gated repository. We can do this by running the following command:

!huggingface-cli login --token YOUR_TOKEN

we are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker.

# 2. Load and prepare the dataset:-
We will use BB-Ultrachat-IndicLingual6-12k an open-source dataset, it is a curated dataset comprising 12,000 multi-turn conversations, which are a subset of the larger HuggingFaceH4/ultrachat_200k dataset from which zephyr-7b model is trained on. These conversations have been evenly distributed across six prominent Indic languages, namely English, Hindi, Tamil, Malayalam, Marathi, and Kannada.
The Indic language data in this dataset was generated by translating the chat data from the HuggingFaceH4/ultrachat_200k dataset using the advanced translation model IndicTrans2 by AI4Bharat.

now, to finetune our model we need to convert the dataset into an instruction format like converting the columns from the BB-Ultrachat-IndicLingual6-12k, messages into processed_text column. I have curated this processed_text column in a new dataset which is https://huggingface.co/datasets/piyushaaryan011/indic_bilingual. Where i have created a #text with the prompt and #response which is the response it should get.
Now, we will try to convert it into tokens using AutoTokenizer.
In addition, to formatting our samples we also want to pack multiple samples to one sequence to have a more efficient training. This means that we are stacking multiple samples to one sequence and split them with an EOS Token. We are going to pack using chunk function.
then , the pack will be converted into {"input_ids": [], "attention_mask": [], "token_type_ids": []}.

After we processed the datasets we are going to use the new FileSystem integration to upload our dataset to S3. We are using the sess.default_bucket(), adjust this if you want to store the dataset in a different S3 bucket. 

Now, we are ready with dataset.



# 3. Fine-Tune zephyr 7B with QLoRA on Amazon SageMaker:

We are going to use the recently introduced method in the paper "QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation" by Tim Dettmers et al. QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance. The TL;DR; of how QLoRA works is:

-> Quantize the pretrained model to 4 bits and freezing it.
-> Attach small, trainable adapter layers. (LoRA)
-> Finetune only the adapter layers, while using the frozen quantized model for context.
We have a run_qlora.py, which implements QLora using PEFT to train our model. The script also merges the LoRA weights into the model weights after training. That way you can use the model as a normal model without any additional code. The model will be temporally offloaded to disk, if it is too large to fit into memory.

In Addition to QLoRA we will leverage the new Flash Attention 2 integrating with Transformers to speed up the training. Flash Attention 2 is a new efficient attention mechanism that is up to 3x faster than the standard attention mechanism.

Now, we will be setting up the hyperparameters:-
from huggingface_hub import HfFolder

 'model_id'->  # pre-trained model
 'dataset_path'->  # path where sagemaker will save training dataset
 'num_train_epochs'->1,  # number of training epochs
 'per_device_train_batch_size'->  # This determines how many examples the model processes at once during training. Smaller batches may lead to better learning, but take more time.

 'gradient_accumulation_steps'-> # This accumulates gradients over multiple steps before updating the model, which can improve memory efficiency.
 'gradient_checkpointing'->  # This reduces memory usage during training, but can slow down the process.
 'bf16'->  # This uses a lower-precision format for model weights, which can speed up training on certain hardware
 'tf32'->  # This uses a higher-precision format for model weights, which can improve accuracy in some cases.
 'learning_rate'->   # learning rate
 'max_grad_norm'-> #  This prevents the model's weights from changing too drastically, helping to keep the learning process stable.

 'warmup_ratio'-> # This gradually increases the learning rate at the beginning of training, which can sometimes improve performance.

 "lr_scheduler_type":"constant",->         #  This sets the strategy for adjusting the learning rate during training.

 'save_strategy': "epoch", ->       #This determines when checkpoints (snapshots of the model's progress) are saved.
"logging_steps": 10,      ->          #This controls how often progress updates are displayed during training.
 'merge_adapters': True,->           # This integrates LoRA (a model compression technique) into the model, potentially reducing its size.
 'use_flash_attn': True,->               # This enables Flash Attention, a faster attention mechanism for transformers.
 'output_dir': '/tmp/run',->                 # output directory, where to save assets during training
                                                    # could be used for checkpointing. The final trained
                                                    # model will always be saved to s3 at the end of training


 In order to create a sagemaker training job we need an HuggingFace Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. The Estimator manages the infrastructure use. Amazon SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at /opt/ml/input/data. Then, it starts the training job by running.




# create the Estimator
huggingface_estimator

We are going to use ml.g5.2xlarge.
entry_point          = 'run_qlora.py',    # train script
source_dir           = '../scripts',      # directory which includes all the files needed for training
 instance_type        = 'ml.g5.2xlarge',   # instances type used for the training job
 instance_count       = 1,                 # the number of instances used for training
 max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)
 base_job_name        = job_name,          # the name of the training job
   role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3
 volume_size          = 300,               # the size of the EBS volume in GB
 transformers_version = '4.28',            # the transformers version used in the training job
 pytorch_version      = '2.0',             # the pytorch_version version used in the training job
 py_version           = 'py310',           # the python version used in the training job
 hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job
 environment          = { "HUGGINGFACE_HUB_CACHE": "/tmp/.cache" }, # set env variable to cache models in /tmp
 disable_output_compression = True         # not compress output to save training time and cost

We can now start our training job, with the .fit() method passing our S3 path to the training script.

Training zephyr-7B, the SageMaker training job took 36,000 seconds, which is about 10 hours. The ml.g5.2xlarge instance we used costs $1.5 per hour for on-demand usage. As a result, the total cost for training our fine-tuned Mistral model was only ~$15.

Now lets make sure SageMaker has successfully uploaded the model to S3. We can use the model_data property of the estimator to get the S3 path to the model. Since we used merge_weights=True and disable_output_compression=True the model is stored as raw files in the S3 bucket.

# 4.Deploy Fine-tuned zephyr 7B on Amazon SageMaker:
Compared to deploying regular Hugging Face models we first need to retrieve the container uri and provide it to our HuggingFaceModel model class with a image_uri pointing to the image. To retrieve the new Hugging Face LLM DLC in Amazon SageMaker, we can use the get_huggingface_llm_image_uri method provided by the sagemaker SDK. This method allows us to retrieve the URI for the desired Hugging Face LLM DLC based on the specified backend, session, region, and version. You can find the available versions here:-
https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-text-generation-inference-containers

We can now create a HuggingFaceModel using the container uri and the S3 path to our model. We also need to set our TGI configuration including the number of GPUs, max input tokens. You can find a full list of configuration optios in the notebook.

We have used ml.g5.xlarge to save the cost for the deployment.

config = {
  'HF_MODEL_ID': "/opt/ml/model",-> # path to where sagemaker stores the model
  'SM_NUM_GPUS': json.dumps(number_of_gpu),-> # Number of GPU used per replica
  'MAX_INPUT_LENGTH': json.dumps(4096),->  # Max length of input text
  'MAX_TOTAL_TOKENS': json.dumps(16000),->  # Max length of the generation (including input text)
}

After we have created the HuggingFaceModel we can deploy it to Amazon SageMaker using the deploy method.
SageMaker will now create our endpoint and deploy the model to it. This can takes a 10-15 minutes.
# 5. Stream Inference Requests from the Deployed Model using aws lambda:

